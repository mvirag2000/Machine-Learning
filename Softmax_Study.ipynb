{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax Study",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3dmawog0FGWU9REMj6NGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvirag2000/Machine-Learning/blob/master/Softmax_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jlzpS-DHj6t",
        "colab_type": "text"
      },
      "source": [
        "# Softmax study\n",
        "First, produce a plausible vector of ReLU outputs, Z.  I am used to (classes x cases), i.e. Z is a column vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haAH4NsQHgeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "np.random.seed(1)\n",
        "features = 6\n",
        "rng = default_rng()\n",
        "Z = rng.standard_normal(features)**2\n",
        "Z = Z.reshape((Z.shape[0],1))\n",
        "print(Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEKjQ_7xLHYg",
        "colab_type": "text"
      },
      "source": [
        "Next, run Z through Softmax to produce A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fKHY2PJLPk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  x_shift = x - np.max(x)\n",
        "  return np.exp(x_shift) / np.sum(np.exp(x_shift)) \n",
        "A = softmax(Z)\n",
        "print(A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhF7iivm3H4y",
        "colab_type": "text"
      },
      "source": [
        "Practice this transform with integers first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39r5PUohQDR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivative(A): # This is how Eli says to backprop SoftMax \n",
        "  # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ \n",
        "  X = np.outer(A, A)\n",
        "  I = np.identity(X.shape[0])\n",
        "  D = A * I - X \n",
        "  return D\n",
        "R = rng.integers(7, size=(1,5))\n",
        "print(R)\n",
        "print(derivative(R))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P6L9BtgQv70",
        "colab_type": "text"
      },
      "source": [
        "Generate a simulated one-hot Y vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-5y9jNRcCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hot = rng.integers(features)\n",
        "Y = np.zeros((features,1))  \n",
        "Y[hot] = 1\n",
        "print(Y) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPr2JVPKSZZ8",
        "colab_type": "text"
      },
      "source": [
        "Now this should be the back propagation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdO-woZc8k0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dA = A - Y\n",
        "print(dA)\n",
        "dZ = np.sum(derivative(dA), axis=0)\n",
        "print(dZ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou9ienfvsyjy",
        "colab_type": "text"
      },
      "source": [
        "That worked great but what if A has multiple cases?  There seems to be no pythonic way to do this, i.e. multiply with axis, so I give up and use a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2juxiTHAs7Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dA_mult = np.hstack((dA, dA, dA)) \n",
        "feat = dA_mult.shape[0]\n",
        "print(dA_mult)  \n",
        "dZ = np.ones((feat, 1))  \n",
        "for case in range(dA_mult.shape[1]):\n",
        "  C = dA_mult[:, case] \n",
        "  dZ_temp = np.sum(derivative(C), axis=0)\n",
        "  dZ_temp = dZ_temp.reshape((feat, 1))   \n",
        "  dZ = np.hstack((dZ, dZ_temp))\n",
        "dZ = np.delete(dZ, 0, axis=1)\n",
        "print(dZ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO2xGhOQY2vz",
        "colab_type": "text"
      },
      "source": [
        "Actually that did not work so well because the whole exercise only succeeds in setting dZ = dA.  That can't be right. "
      ]
    }
  ]
}
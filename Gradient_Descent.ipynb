{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNchLmh5h75OCXrO5v8Iil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvirag2000/Machine-Learning/blob/master/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFAzbuzbfqS",
        "colab_type": "text"
      },
      "source": [
        "# Simple Linear Regression using Gradient Descent\n",
        "In this notebook we will demonstrate the principal of gradient descent in isolation, without the other machinery of machine learning, using a well understood example that can also be solved analytically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZspJZTOkcoSC",
        "colab_type": "text"
      },
      "source": [
        "These figures are from the Westwood Company exercise, Neter Wasserman Kutner Third Edition.  They are supposed to represent labor hours (Y) as a function of production lot size (X).  The goal is to fit a \"least squares line\" to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke7xtagnbJDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([30, 20, 60, 80, 40, 50, 60, 30, 70, 60], dtype=float)\n",
        "y = np.array([73, 50, 128, 170, 87, 108, 135, 69, 148, 132], dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_s0424VdqDA",
        "colab_type": "text"
      },
      "source": [
        "First, let's solve the problem analytically using the standard formula.  Note that, while x_mean is a real number, X and Y are 1D arrays (vectors).  Python is good for machine learning because it automatically handles vector arithmetic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6RGqERNeK1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_mean = np.average(x)\n",
        "y_mean = np.average(y)\n",
        "SSxy = sum((x - x_mean) * (y - y_mean))\n",
        "SSxx = sum((x - x_mean)**2)\n",
        "b1 = SSxy / SSxx\n",
        "b0 = y_mean - b1 * x_mean\n",
        "y_pred = b1 * x + b0 \n",
        "SSE = sum((y_pred - y)**2)\n",
        "print(\"The slope coefficient is \" + str(b1))\n",
        "print(\"The y-intercept is \" + str(b0))\n",
        "print(\"The sum of squared errors is \" + str(SSE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se1fWzRYfOhi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The vector treatment of this problem is given in NWK Chap. 6. \n",
        "Here is the X-Y plot with the fitted line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtg_Jr0_fStx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(x,y,color='b',marker='x')\n",
        "ax.plot(x,y_pred,color='r')\n",
        "ax.axes.set_ylim(0,200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVSnAU_4oi42",
        "colab_type": "text"
      },
      "source": [
        "Now let's set up to solve it iteratively.  We start by normalizing the data.  This is because the scales of X and Y are arbitrary.  If we don't normalize, we may have a distorted surface where the minimum is hard to find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0G0U-LEovRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_stdev = np.sqrt(np.average((x - x_mean)**2))\n",
        "y_stdev = np.sqrt(np.average((y - y_mean)**2))\n",
        "x_norm = (x - x_mean) /x_stdev\n",
        "y_norm = (y - y_mean) /y_stdev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV3xTusPtvO4",
        "colab_type": "text"
      },
      "source": [
        "Because this is an old problem, everyone knows that SSE is a paraboloid in b1, b0. The \"objective\" is to find (b1, b0) for which SSE is minimized.  Machine learning uses other objectives, like RMSE, but SSE is traditional here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do4Pz2zYucKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "def SSE(b1, b0, x, y):\n",
        "    y_pred = b1 * x + b0 \n",
        "    return sum((y_pred - y)**2)\n",
        "SSE_plot = np.empty(shape=[50, 50])\n",
        "b1_plot = np.linspace(-3, 5, 50, dtype = float)\n",
        "b0_plot = np.linspace(-5, 5, 50, dtype = float)\n",
        "B1, B0 = np.meshgrid(b1_plot, b0_plot)\n",
        "for i in range(50):\n",
        "    for j in range(50): \n",
        "        SSE_plot[j, i] = SSE(b1_plot[i], b0_plot[j], x_norm, y_norm)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(B1, B0, SSE_plot, cmap=cm.jet) \n",
        "ax.set_xlabel('slope (B1) axis')\n",
        "ax.set_ylabel('intercept (B0) axis')\n",
        "ax.set_zlabel('SSE axis')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chlr-Il1YYiY",
        "colab_type": "text"
      },
      "source": [
        "To see why we normalize, try replotting that paraboloid with x and y instead of x_norm and y_norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcPyNr8wkx0B",
        "colab_type": "text"
      },
      "source": [
        "Next, we roll down the grade into the hollow.  This is the everyday usage of the English word \"gradient.\"  We happen to have formulas for the db1 and db2 gradients, which we apply iteratively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlifZKqvlZAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b1 = 5\n",
        "b0 = 5\n",
        "n = x_norm.shape[0]\n",
        "learning_rate = 0.0015\n",
        "for i in range(1, 500):\n",
        "  y_pred = b1 * x_norm + b0\n",
        "  S = np.sum((y_pred - y_norm)**2)\n",
        "  if i % 50 == 0:\n",
        "    print(\"Cost after iteration {}: {}\".format(i, np.squeeze(S)))\n",
        "  db1 = b1 * np.sum(x_norm**2) + b0 * np.sum(x_norm) - np.sum(x_norm * y_norm) \n",
        "  db0 = n * b0 + b1 * np.sum(x_norm) - np.sum(y_norm) \n",
        "  b1 = b1 - learning_rate * db1\n",
        "  b0 = b0 - learning_rate * db0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhpl5GLs6Yey",
        "colab_type": "text"
      },
      "source": [
        "That seems to have found a minimum around 0.045 so let's un-norm the b's and see how they compare to the known solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9NTAq7S6mA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b1 = b1 * y_stdev / x_stdev\n",
        "b0 = y_mean - b1 * x_mean\n",
        "y_pred = b1 * x + b0 # Back to the original X and Y \n",
        "SSE = sum((y_pred - y)**2)\n",
        "print(\"The slope coefficient is %1.2f\" % b1)\n",
        "print(\"The y-intercept is %1.2f\" % b0) \n",
        "print(\"The sum of squared errors is %2.2f\" % SSE) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipx9n3OQdFA4",
        "colab_type": "text"
      },
      "source": [
        "These are pretty good estimates:\n",
        "\n",
        "The slope coefficient is 2.00<br>\n",
        "The y-intercept is 9.79<br>\n",
        "The sum of squared errors is 60.06\n",
        "\n",
        "Obviously you wouldn't really use gradient descent to find the minimum of a paraboloid, but it's the thing to do when you're optimizing hundreds of parameters in multiple dimensions. "
      ]
    }
  ]
}